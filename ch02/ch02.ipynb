{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 2 - First Steps with LangChain\n",
    "\n"
   ],
   "id": "777e208c3cd66126"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ChatDeepSeek example\n",
    "\n",
    "Taken from [langchain-deepseek](https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html) documentation"
   ],
   "id": "e5af445acf0c3a6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T01:50:55.564489Z",
     "start_time": "2025-07-15T01:50:47.382106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # load environment variables from .env file\n",
    "\n",
    "# instantiate the llm\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.environ.get(\"DEEPSEEK_API_KEY\"))\n",
    "\n",
    "# construct a prompt\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Spanish\"),\n",
    "    (\"human\", \"I love programming.\")\n",
    "]\n",
    "\n",
    "# pass the prompt to the llm and get a response\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ],
   "id": "47e4933507c8a409",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Me encanta programar.  \\n\\n*(Alternative options depending on context:)*  \\n- **Amo programar.** (stronger emphasis)  \\n- **Disfruto mucho programando.** (\"I really enjoy programming\")  \\n\\nLet me know if you\\'d like a regional variation (e.g., Latin America vs. Spain) or a different tone!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 19, 'total_tokens': 90, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 19}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': 'db3b6cc4-4e7b-4ebf-853c-9b8ee4d94959', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--0fc05e59-ffb8-489c-b02f-323407d02631-0' usage_metadata={'input_tokens': 19, 'output_tokens': 71, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Development testing\n",
    "\n",
    "During development, you might want to test your application without making an actual API calls. LangChain provides ```FakeListLLM``` for this purpose:"
   ],
   "id": "eab5ba80289e54a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T02:03:30.147364Z",
     "start_time": "2025-07-15T02:03:30.131276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "\n",
    "# create a fake LLM that always returns the same response\n",
    "\n",
    "fake_llm = FakeListLLM(responses=[\"Hello There\"])\n",
    "\n",
    "response = fake_llm.invoke(\"Any input will return 'Hello There'\")\n",
    "print(response)"
   ],
   "id": "26e6be801b5d9c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello There\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Working with chat models",
   "id": "162ca542fbd6f5af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T02:25:01.527390Z",
     "start_time": "2025-07-15T02:24:41.148978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # load environment variables from .env file\n",
    "\n",
    "# instantiate llm\n",
    "chat = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.environ.get(\"DEEPSEEK_API_KEY\"))\n",
    "\n",
    "# construct prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful programming assistant.\"),\n",
    "    HumanMessage(content=\"Write a Python function to calculate factorial.\"),\n",
    "]\n",
    "\n",
    "# invoke the llm\n",
    "response = chat.invoke(messages)\n",
    "print(response)"
   ],
   "id": "5b533ccb5d4e0e82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hereâ€™s a Python function to calculate the factorial of a non-negative integer using both iterative and recursive approaches:\\n\\n### 1. Iterative Approach:\\n```python\\ndef factorial_iterative(n):\\n    \"\"\"\\n    Calculate the factorial of a non-negative integer n using iteration.\\n    \\n    Args:\\n        n (int): Non-negative integer\\n    \\n    Returns:\\n        int: Factorial of n\\n    \"\"\"\\n    if n < 0:\\n        raise ValueError(\"Factorial is not defined for negative numbers.\")\\n    result = 1\\n    for i in range(1, n + 1):\\n        result *= i\\n    return result\\n```\\n\\n### 2. Recursive Approach:\\n```python\\ndef factorial_recursive(n):\\n    \"\"\"\\n    Calculate the factorial of a non-negative integer n using recursion.\\n    \\n    Args:\\n        n (int): Non-negative integer\\n    \\n    Returns:\\n        int: Factorial of n\\n    \"\"\"\\n    if n < 0:\\n        raise ValueError(\"Factorial is not defined for negative numbers.\")\\n    if n == 0 or n == 1:\\n        return 1\\n    return n * factorial_recursive(n - 1)\\n```\\n\\n### Example Usage:\\n```python\\nn = 5\\nprint(f\"Iterative factorial of {n}: {factorial_iterative(n)}\")  # Output: 120\\nprint(f\"Recursive factorial of {n}: {factorial_recursive(n)}\")  # Output: 120\\n```\\n\\n### Notes:\\n- Both functions handle the base case where `n = 0` or `n = 1` (factorial is `1`).\\n- They raise a `ValueError` if `n` is negative since factorial is not defined for negative numbers.\\n- The iterative approach is generally more efficient for large `n` because it avoids recursion stack limits.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 375, 'prompt_tokens': 18, 'total_tokens': 393, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 18}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': 'b45f44cd-8e34-4691-938b-56997ca96ee2', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--0ad650cc-85b3-499f-819c-c86b36c2084c-0' usage_metadata={'input_tokens': 18, 'output_tokens': 375, 'total_tokens': 393, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
